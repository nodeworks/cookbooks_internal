{
  "providing": {
  },
  "dependencies": {
    "bootstrap": ">= 0.0.0",
    "rightscale": ">= 0.0.0",
    "repo": ">= 0.0.0",
    "block_device": ">= 0.0.0",
    "sys_dns": ">= 0.0.0",
    "sys_firewall": ">= 0.0.0"
  },
  "platforms": {
  },
  "groupings": {
  },
  "long_description": "Description\n===========\nInstall and Configure Apache Hadoop 1.03 \n\nRequirements\n============\n\n* Requires a VM launched from a RightScale managed RightImage.\n\n== KNOWN LIMITATIONS:\n\nThere are no known limitations.\n\nAttributes\n==========\n\n* See <tt>metadata.rb</tt> for the list of attributes and their description.\n\nUsage\n=====\n\nIf your cloud supports Security Groups, i.e. Amazon EC2, set the rules to allow \nports 8020, 50000-50100 (or the ports you use in the inputs) between namenode \nand datanodes.\n\nThis cookbook has two features\n\n1. Launch one Namenode (master server) and unlimited Datanode (slave) servers.\nUse the ServerTemplate inputs to select which type of server you will launch.  \nThe default server launched is a NameNode, simply change the attribute hadoop/node/type input\nto datanode to launch a datanode.  Clone as many datanode servers you need or put datanode\nservers in a ServerArray to launch as many as you need.\n\n2. Run MapReduce commands using java classes.  Data is downloaded from your ROS\nand copied into your HDFS input directory.  After your MapReduce program is run, the \noutput data is uploaded to your ROS.\n\n\n= LICENSE:\n\nCopyright RightScale, Inc. All rights reserved.  All access and use subject to\nthe RightScale Terms of Service available at http://www.rightscale.com/terms.php\nand, if applicable, other agreements such as a RightScale Master Subscription\nAgreement.\n\n",
  "suggestions": {
  },
  "version": "1.0.0",
  "recipes": {
    "hadoop::do_attach_request": "Attach datanodes to the namenode during boot.",
    "hadoop::install": "Install Hadoop",
    "hadoop::do_cleanup": "Run on Namenode. Remove working directories and MapReduce input/output directories",
    "hadoop::do_allow": "Add firewall rules to allow namenode and datanode connections within the cluster",
    "hadoop::do_detach_request": "Detach datanodes from the namenode when terminating the server.",
    "hadoop::default": "Install, configure and initialize Hadoop",
    "hadoop::do_deny": "Remove firewall rules for nodenames and datanodes. ",
    "hadoop::do_attach_all": "Attach datanodes to the namenode run from cron on the namenode",
    "hadoop::do_stop": "Stop Hadoop",
    "hadoop::handle_attach": "Handle Attach",
    "hadoop::do_start": "Start Hadoop",
    "hadoop::do_detach_all": "Detach datanodes from the namenode.  Runs from cron on the namenode",
    "hadoop::handle_detach": "Handle Detach",
    "hadoop::do_restart": "Restart Hadoop",
    "hadoop::do_data_import": "Run on Namenode. Download data from a cloud provider ROS and copy it into the Hadoop HDFS.",
    "hadoop::do_init": "Format the namenode",
    "hadoop::do_config": "Configure Hadoop",
    "hadoop::do_map_reduce": "Run on Namenode, Run MapReduce command on data imported and upload it to the cloud provider ROS."
  },
  "replacing": {
  },
  "license": "All rights reserved",
  "recommendations": {
  },
  "maintainer_email": "premium@rightscale.com",
  "description": "Install and Configure Apache Hadoop",
  "attributes": {
    "mapreduce/destination": {
      "calculated": false,
      "choice": [

      ],
      "type": "string",
      "recipes": [
        "hadoop::do_data_import",
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "display_name": "Location of all download and compiled files for Hadoop MapReduce command",
      "description": "Location where data file will be placed.",
      "default": "/mapreduce"
    },
    "mapreduce/data/storage_account_id": {
      "calculated": false,
      "choice": [

      ],
      "type": "string",
      "recipes": [
        "hadoop::do_data_import",
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "display_name": "Data Storage Account ID",
      "description": "In order to download the data file to the specified cloud \n   storage location, you need to provide cloud authentication credentials. \n   For Amazon S3, use your Amazon access key ID (e.g., cred:AWS_ACCESS_KEY_ID). \n   For Rackspace Cloud Files, use your Rackspace login username (e.g., cred:RACKSPACE_USERNAME)."
    },
    "mapreduce/command": {
      "calculated": false,
      "choice": [

      ],
      "type": "string",
      "recipes": [
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "display_name": "Hadoop MapReduce class and arguments to run.",
      "description": "The class and arguments to run the MapReduce job.  This input is \n  appended to the end of the 'hadoop jar jarfile' command.  \n  Example: org.myorg.MyMapReduce input output"
    },
    "mapreduce/input": {
      "calculated": false,
      "choice": [

      ],
      "type": "string",
      "recipes": [
        "hadoop::do_data_import"
      ],
      "required": "optional",
      "display_name": "Hadoop HDFS Input Directory",
      "description": "Directory created in HDFS for input data used by MapReduce command",
      "default": "input"
    },
    "rightscale/public_ssh_key": {
      "calculated": false,
      "choice": [

      ],
      "type": "string",
      "recipes": [
        "hadoop::default",
        "hadoop::do_init"
      ],
      "required": "required",
      "display_name": "Public SSH Key ",
      "description": "Hadoop needs a public ssh key which it can use to ssh to \nsystems in it's cluster. This key should also match the private key supplied in ssh/private_ssh_key"
    },
    "mapreduce/cleanup": {
      "calculated": false,
      "choice": [
        "yes",
        "no"
      ],
      "type": "string",
      "recipes": [
        "hadoop::do_data_import",
        "hadoop::do_cleanup"
      ],
      "required": "optional",
      "display_name": "Delete the destination and Hadoop input/output directories",
      "description": "Removes all the working directories.",
      "default": "no"
    },
    "hadoop/datanode/ipc/port": {
      "calculated": false,
      "choice": [

      ],
      "type": "string",
      "recipes": [
        "hadoop::do_allow",
        "hadooop:do_deny"
      ],
      "required": "optional",
      "display_name": "Datanode ipc firewall port ",
      "description": "Set the firewall port used by the datanode ipc address",
      "default": "50020"
    },
    "mapreduce/data/prefix": {
      "calculated": false,
      "choice": [

      ],
      "type": "string",
      "recipes": [
        "hadoop::do_data_import"
      ],
      "required": "optional",
      "display_name": "Data file name prefix to download",
      "description": "The prefix that will be used to name/locate.  If there are \n  multiple version, use a timestamp in the filename.  The newest will be \n  picked up first.  Should be a .tar.gz file"
    },
    "hadoop/node/type": {
      "calculated": false,
      "choice": [
        "namenode",
        "datanode"
      ],
      "type": "string",
      "recipes": [
        "hadoop::default",
        "hadoop::do_init",
        "hadoop::do_config",
        "hadoop::do_data_import",
        "hadoop::do_cleanup"
      ],
      "required": "optional",
      "display_name": "Hadoop node type",
      "description": "Hadoop node type, used for managing slaves and masters",
      "default": "namenode"
    },
    "mapreduce/output": {
      "calculated": false,
      "choice": [

      ],
      "type": "string",
      "recipes": [
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "display_name": "Hadoop HDFS Output Directory",
      "description": "Output directory to place data when MapReduce command is run. ",
      "default": "output"
    },
    "hadoop/datanode/http/port": {
      "calculated": false,
      "choice": [

      ],
      "type": "string",
      "recipes": [
        "hadoop::do_allow",
        "hadooop:do_deny"
      ],
      "required": "optional",
      "display_name": "Datanode http firewall port ",
      "description": "Set the firewall port used by the datanode http server",
      "default": "50075"
    },
    "mapreduce/data/storage_account_secret": {
      "calculated": false,
      "choice": [

      ],
      "type": "string",
      "recipes": [
        "hadoop::do_data_import",
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "display_name": "Data Storage Account Secret",
      "description": "In order to get the data file to the specified cloud storage \n   location, you will need to provide cloud authentication credentials. \n   For Amazon S3, use your AWS secret access key (e.g., cred:AWS_SECRET_ACCESS_KEY). \n   For Rackspace Cloud Files, use your Rackspace account API key (e.g., cred:RACKSPACE_AUTH_KEY)."
    },
    "hadoop/datanode/address/port": {
      "calculated": false,
      "choice": [

      ],
      "type": "string",
      "recipes": [
        "hadoop::do_allow",
        "hadooop:do_deny"
      ],
      "required": "optional",
      "display_name": "Datanode address firewall port",
      "description": "Set the firewall port used by the datanode address",
      "default": "50010"
    },
    "hadoop/namenode/http/port": {
      "calculated": false,
      "choice": [

      ],
      "type": "string",
      "recipes": [
        "hadoop::do_allow",
        "hadooop:do_deny"
      ],
      "required": "optional",
      "display_name": "Namenode http firewall port",
      "description": "Set the firewall port used by the namenode http server",
      "default": "50070"
    },
    "mapreduce/data/container": {
      "calculated": false,
      "choice": [

      ],
      "type": "string",
      "recipes": [
        "hadoop::do_data_import",
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "display_name": "Data file container",
      "description": "The cloud storage location where the data file will be downloaded from and uploaded to. \n  For Amazon S3, use the bucket name. For Rackspace Cloud Files, use the container name."
    },
    "mapreduce/data/storage_account_provider": {
      "calculated": false,
      "choice": [
        "s3",
        "cloudfiles",
        "cloudfilesuk",
        "SoftLayer_Dallas",
        "SoftLayer_Singapore",
        "SoftLayer_Amsterdam"
      ],
      "type": "string",
      "recipes": [
        "hadoop::do_data_import",
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "display_name": "Data Storage Account Provider",
      "description": "Location where the data file will be imported from and uploaded to. \n   Used by dump recipes to back up to Amazon S3 or Rackspace Cloud Files."
    },
    "hadoop/dfs/replication": {
      "calculated": false,
      "choice": [

      ],
      "type": "string",
      "recipes": [
        "hadoop::default",
        "hadoop::do_config"
      ],
      "required": "required",
      "display_name": "Hadoop namenode dfs.replicaton property ",
      "description": "Hadoop namenode dfs.replicaton property"
    },
    "mapreduce/compile": {
      "calculated": false,
      "choice": [

      ],
      "type": "string",
      "recipes": [
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "display_name": "Hadoop MapReduce source files to compile.",
      "description": "Source files to complile. Example: org/myorg/*.java org/myorg/foo/*.java"
    },
    "mapreduce/data/output_prefix": {
      "calculated": false,
      "choice": [

      ],
      "type": "string",
      "recipes": [
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "display_name": "Prefix of output file name to upload",
      "description": "The prefix of the output filename.  Output file is tar.gz'd"
    },
    "mapreduce/name": {
      "calculated": false,
      "choice": [

      ],
      "type": "string",
      "recipes": [
        "hadoop::do_map_reduce"
      ],
      "required": "optional",
      "display_name": "Hadoop MapReduce program name",
      "description": "Used as namespace for paths and commands:  Example MyMapReduce"
    },
    "hadoop/namenode/address/port": {
      "calculated": false,
      "choice": [

      ],
      "type": "string",
      "recipes": [
        "hadoop::do_allow",
        "hadooop:do_deny"
      ],
      "required": "optional",
      "display_name": "Namenode firewall port",
      "description": "Set the firewall port used by the namenode",
      "default": "8020"
    }
  },
  "maintainer": "RightScale Inc",
  "name": "hadoop",
  "conflicting": {
  }
}